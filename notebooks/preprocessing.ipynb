{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5c0f2f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143405/3146432059.py:5: DtypeWarning: Columns (11,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"../data/raw/elonmusk_raw.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1602029 entries, 0 to 461420\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count    Dtype  \n",
      "---  ------         --------------    -----  \n",
      " 0   ts_event       1602029 non-null  object \n",
      " 1   rtype          1602029 non-null  int64  \n",
      " 2   publisher_id   1602029 non-null  int64  \n",
      " 3   instrument_id  1602029 non-null  int64  \n",
      " 4   open           1602029 non-null  float64\n",
      " 5   high           1602029 non-null  float64\n",
      " 6   low            1602029 non-null  float64\n",
      " 7   close          1602029 non-null  float64\n",
      " 8   volume         1602029 non-null  int64  \n",
      " 9   symbol         1602029 non-null  object \n",
      "dtypes: float64(4), int64(4), object(2)\n",
      "memory usage: 134.4+ MB\n",
      "None\n",
      " ------------------------------------------------------ \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55099 entries, 0 to 55098\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   id                        55099 non-null  int64  \n",
      " 1   url                       55099 non-null  object \n",
      " 2   twitterUrl                55099 non-null  object \n",
      " 3   fullText                  55099 non-null  object \n",
      " 4   retweetCount              55009 non-null  float64\n",
      " 5   replyCount                54297 non-null  float64\n",
      " 6   likeCount                 55009 non-null  float64\n",
      " 7   quoteCount                54271 non-null  float64\n",
      " 8   viewCount                 34455 non-null  float64\n",
      " 9   createdAt                 55099 non-null  object \n",
      " 10  bookmarkCount             54271 non-null  float64\n",
      " 11  isReply                   54271 non-null  object \n",
      " 12  inReplyToId               39497 non-null  float64\n",
      " 13  conversationId            54297 non-null  float64\n",
      " 14  inReplyToUserId           39521 non-null  float64\n",
      " 15  inReplyToUsername         39521 non-null  object \n",
      " 16  isPinned                  54271 non-null  object \n",
      " 17  isRetweet                 55009 non-null  object \n",
      " 18  isQuote                   54983 non-null  object \n",
      " 19  isConversationControlled  54271 non-null  object \n",
      " 20  possiblySensitive         4921 non-null   object \n",
      " 21  quoteId                   7541 non-null   float64\n",
      " 22  quote                     7273 non-null   object \n",
      " 23  retweet                   1092 non-null   object \n",
      "dtypes: float64(10), int64(1), object(13)\n",
      "memory usage: 10.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stock_price_by_min = pd.read_csv(\"../data/raw/TSLA_5_year_by_min.csv\")\n",
    "stock_price_by_min_2018_2020 = pd.read_csv(\"../data/raw/TSLA_2018_to_2020.csv\")\n",
    "tweets = pd.read_csv(\"../data/raw/elonmusk_raw.csv\")\n",
    "\n",
    "stock_price_by_min = pd.concat(\n",
    "    [stock_price_by_min, stock_price_by_min_2018_2020]\n",
    ").drop_duplicates()\n",
    "\n",
    "print(stock_price_by_min.info())\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(tweets.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b06344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original tweets: 6857 / 55099\n",
      "1330\n",
      "1326\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1326 entries, 0 to 1329\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   fullText           1326 non-null   object \n",
      " 1   day_sin            1326 non-null   float64\n",
      " 2   day_cos            1326 non-null   float64\n",
      " 3   hour_sin           1326 non-null   float64\n",
      " 4   hour_cos           1326 non-null   float64\n",
      " 5   avg_last_5_prices  1326 non-null   float64\n",
      " 6   bps_change         1326 non-null   float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 82.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullText</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>avg_last_5_prices</th>\n",
       "      <th>bps_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh and uh short burn of the century comin soon...</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-19.728035</td>\n",
       "      <td>1.056859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rococo basilisk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.268231</td>\n",
       "      <td>44.054334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Night at the museum. G has mad skillzsz. Me no...</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>8.836035</td>\n",
       "      <td>11.294180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@NannySoleil @CatherineUllo We sketched concep...</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>7.653910</td>\n",
       "      <td>19.966722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It‚Äôs super messed up that a Tesla crash result...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.984107</td>\n",
       "      <td>-2.402773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fullText   day_sin   day_cos  \\\n",
       "0  Oh and uh short burn of the century comin soon... -0.433884 -0.900969   \n",
       "1                                    Rococo basilisk  0.000000  1.000000   \n",
       "2  Night at the museum. G has mad skillzsz. Me no...  0.974928 -0.222521   \n",
       "3  @NannySoleil @CatherineUllo We sketched concep...  0.974928 -0.222521   \n",
       "4  It‚Äôs super messed up that a Tesla crash result...  0.000000  1.000000   \n",
       "\n",
       "   hour_sin  hour_cos  avg_last_5_prices  bps_change  \n",
       "0 -0.258819 -0.965926         -19.728035    1.056859  \n",
       "1 -0.965926  0.258819           1.268231   44.054334  \n",
       "2 -0.965926 -0.258819           8.836035   11.294180  \n",
       "3 -0.965926 -0.258819           7.653910   19.966722  \n",
       "4 -0.866025  0.500000          10.984107   -2.402773  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets construct the dataset to predict the stock price in the next X minutes after a\n",
    "# tweet is posted\n",
    "import numpy as np\n",
    "\n",
    "NUM_MINUTES_AHEAD = 5\n",
    "\n",
    "tweet_wanted_attributes = [\"createdAt\", \"isRetweet\", \"isReply\", \"isQuote\", \"fullText\"]\n",
    "stock_wanted_attributes = [\"ts_event\", \"open\", \"close\"]\n",
    "\n",
    "# filtering for only original tweets\n",
    "X = tweets[tweet_wanted_attributes]\n",
    "X = X[(X[\"isRetweet\"] == False) & (X[\"isReply\"] == False) & (X[\"isQuote\"] == False)]\n",
    "print(f\"Number of original tweets: {len(X)} / {len(tweets)}\")\n",
    "\n",
    "# converting createdAt to datetime\n",
    "X[\"createdAt\"] = pd.to_datetime(X[\"createdAt\"])\n",
    "\n",
    "# featurizing day of week and hour of day using sin and cos transformations\n",
    "X[\"day_sin\"] = X[\"createdAt\"].dt.dayofweek.apply(lambda x: np.sin(2 * np.pi * x / 7))\n",
    "X[\"day_cos\"] = X[\"createdAt\"].dt.dayofweek.apply(lambda x: np.cos(2 * np.pi * x / 7))\n",
    "X[\"hour_sin\"] = X[\"createdAt\"].dt.hour.apply(lambda x: np.sin(2 * np.pi * x / 24))\n",
    "X[\"hour_cos\"] = X[\"createdAt\"].dt.hour.apply(lambda x: np.cos(2 * np.pi * x / 24))\n",
    "\n",
    "# lets now get the stock prices NUM_MINUTES_AHEAD after each tweet\n",
    "stock_price_by_min[\"ts_event\"] = pd.to_datetime(stock_price_by_min[\"ts_event\"])\n",
    "y = stock_price_by_min[stock_wanted_attributes]\n",
    "\n",
    "# create window start and end times to join on\n",
    "X[\"window_start_time\"] = X[\"createdAt\"].dt.ceil(\"min\")\n",
    "X[\"window_end_time\"] = X[\"window_start_time\"] + pd.Timedelta(minutes=NUM_MINUTES_AHEAD)\n",
    "\n",
    "X = X.sort_values(\"createdAt\").reset_index(drop=True)\n",
    "y = y.sort_values(\"ts_event\").reset_index(drop=True)\n",
    "\n",
    "# filter X to only have tweets that have stock price data after them\n",
    "X = X[y[\"ts_event\"].min() <= X[\"window_start_time\"]]\n",
    "# filter to weekdays and market hours (data is in UTC)\n",
    "# this range is a little wider to account for Daylight Savings Time changes\n",
    "X = X[\n",
    "    (X[\"createdAt\"].dt.dayofweek < 5)\n",
    "    & (X[\"createdAt\"].dt.hour >= 13)\n",
    "    & (X[\"createdAt\"].dt.hour < 21)\n",
    "]\n",
    "\n",
    "# join to get stock prices at the start of the window\n",
    "data = pd.merge_asof(\n",
    "    X, y, left_on=\"window_start_time\", right_on=\"ts_event\", direction=\"forward\"\n",
    ")\n",
    "# join to get stock prices at the end of the window\n",
    "data = pd.merge_asof(\n",
    "    data,\n",
    "    y,\n",
    "    left_on=\"window_end_time\",\n",
    "    right_on=\"ts_event\",\n",
    "    direction=\"forward\",\n",
    "    suffixes=(\"_start\", \"_end\"),\n",
    ")\n",
    "\n",
    "# 1. Use the sorted y for the rolling average calculation.\n",
    "y_for_merge = y.set_index(\"ts_event\").copy()\n",
    "\n",
    "# 2. Calculate the rolling average of the PREVIOUS 5 minutes.\n",
    "# If this is still constant, the issue is in the 'y' DataFrame itself.\n",
    "y_for_merge[\"avg_last_5_prices\"] = (\n",
    "    y_for_merge[\"close\"].rolling(window=\"5min\", closed=\"left\", min_periods=1).mean()\n",
    ")\n",
    "# 3. Merge this new feature back into 'data'.\n",
    "data = pd.merge_asof(\n",
    "    data,\n",
    "    y_for_merge[\"avg_last_5_prices\"].reset_index(),\n",
    "    left_on=\"window_start_time\",\n",
    "    right_on=\"ts_event\",\n",
    "    direction=\"backward\",\n",
    ")\n",
    "\n",
    "data[\"avg_last_5_prices\"] = (\n",
    "    10000 * (data[\"avg_last_5_prices\"] - data[\"open_start\"]) / data[\"open_start\"]\n",
    ")\n",
    "\n",
    "# calculate the % change and convert to basis points (0.01%) so we dont have to deal\n",
    "# with small decimals\n",
    "data[\"bps_change\"] = (\n",
    "    10000 * (data[\"close_end\"] - data[\"open_start\"]) / data[\"open_start\"]\n",
    ")\n",
    "\n",
    "# 4. Drop the extra ts_event column created by the merge_asof\n",
    "data = data.drop(columns=[\"ts_event\"])\n",
    "\n",
    "# drop unnecessary columns\n",
    "data = data.drop(\n",
    "    columns=[\n",
    "        \"createdAt\",\n",
    "        \"isRetweet\",\n",
    "        \"isReply\",\n",
    "        \"isQuote\",\n",
    "        \"ts_event_start\",\n",
    "        \"ts_event_end\",\n",
    "        \"open_start\",\n",
    "        \"open_end\",\n",
    "        \"close_start\",\n",
    "        \"close_end\",\n",
    "        \"window_start_time\",\n",
    "        \"window_end_time\",\n",
    "    ]\n",
    ")\n",
    "data.dropna(inplace=True)\n",
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "03d1d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:05<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding tensor: (1326, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we need to vectorize the text. Let's use a pre-trained embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode the full text to get embeddings - embedding dim is 384\n",
    "embeddings = model.encode(data[\"fullText\"].tolist(), show_progress_bar=True)\n",
    "print(\"Shape of embedding tensor:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef6cb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_380</th>\n",
       "      <th>emb_381</th>\n",
       "      <th>emb_382</th>\n",
       "      <th>emb_383</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>avg_last_5_prices</th>\n",
       "      <th>bps_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001008</td>\n",
       "      <td>0.052315</td>\n",
       "      <td>-0.009976</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>0.043964</td>\n",
       "      <td>-0.011720</td>\n",
       "      <td>-0.076216</td>\n",
       "      <td>-0.042937</td>\n",
       "      <td>-0.015438</td>\n",
       "      <td>-0.032219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>-0.039692</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.026120</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-19.728035</td>\n",
       "      <td>1.056859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>-0.065066</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>-0.029911</td>\n",
       "      <td>-0.025508</td>\n",
       "      <td>0.067164</td>\n",
       "      <td>-0.005198</td>\n",
       "      <td>-0.059973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007372</td>\n",
       "      <td>-0.064891</td>\n",
       "      <td>0.070080</td>\n",
       "      <td>0.054620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.268231</td>\n",
       "      <td>44.054334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.113694</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>-0.036774</td>\n",
       "      <td>-0.022655</td>\n",
       "      <td>-0.058566</td>\n",
       "      <td>-0.035179</td>\n",
       "      <td>0.050456</td>\n",
       "      <td>-0.056513</td>\n",
       "      <td>-0.025926</td>\n",
       "      <td>-0.040562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101047</td>\n",
       "      <td>0.040583</td>\n",
       "      <td>-0.122994</td>\n",
       "      <td>-0.003087</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>8.836035</td>\n",
       "      <td>11.294180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.079638</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>-0.010374</td>\n",
       "      <td>-0.044825</td>\n",
       "      <td>0.038043</td>\n",
       "      <td>-0.031383</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036524</td>\n",
       "      <td>0.021378</td>\n",
       "      <td>-0.023749</td>\n",
       "      <td>0.066412</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>7.653910</td>\n",
       "      <td>19.966722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020206</td>\n",
       "      <td>-0.010676</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.060031</td>\n",
       "      <td>0.089319</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>-0.034083</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>-0.035500</td>\n",
       "      <td>0.059870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>0.081259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.984107</td>\n",
       "      <td>-2.402773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 390 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0 -0.001008  0.052315 -0.009976 -0.014350  0.043964 -0.011720 -0.076216   \n",
       "1 -0.079920 -0.016484 -0.065066  0.012709  0.020921 -0.029911 -0.025508   \n",
       "2 -0.113694  0.040588 -0.036774 -0.022655 -0.058566 -0.035179  0.050456   \n",
       "3 -0.079638 -0.041545 -0.010374 -0.044825  0.038043 -0.031383  0.033809   \n",
       "4 -0.020206 -0.010676  0.045836  0.060031  0.089319  0.014100 -0.034083   \n",
       "\n",
       "      emb_7     emb_8     emb_9  ...   emb_380   emb_381   emb_382   emb_383  \\\n",
       "0 -0.042937 -0.015438 -0.032219  ...  0.016286 -0.039692  0.012335  0.026120   \n",
       "1  0.067164 -0.005198 -0.059973  ... -0.007372 -0.064891  0.070080  0.054620   \n",
       "2 -0.056513 -0.025926 -0.040562  ...  0.101047  0.040583 -0.122994 -0.003087   \n",
       "3  0.011510  0.006959  0.023301  ...  0.036524  0.021378 -0.023749  0.066412   \n",
       "4  0.032532 -0.035500  0.059870  ...  0.012382  0.012327 -0.004536  0.081259   \n",
       "\n",
       "    day_sin   day_cos  hour_sin  hour_cos  avg_last_5_prices  bps_change  \n",
       "0 -0.433884 -0.900969 -0.258819 -0.965926         -19.728035    1.056859  \n",
       "1  0.000000  1.000000 -0.965926  0.258819           1.268231   44.054334  \n",
       "2  0.974928 -0.222521 -0.965926 -0.258819           8.836035   11.294180  \n",
       "3  0.974928 -0.222521 -0.965926 -0.258819           7.653910   19.966722  \n",
       "4  0.000000  1.000000 -0.866025  0.500000          10.984107   -2.402773  \n",
       "\n",
       "[5 rows x 390 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a dataframe for the embeddings\n",
    "embedding_cols = [f\"emb_{i}\" for i in range(embeddings.shape[1])]\n",
    "\n",
    "# Add as separate columns (concate horizontally matching on same index)\n",
    "embedding_df = pd.DataFrame(embeddings, columns=embedding_cols, index=data.index)\n",
    "data = pd.concat([embedding_df, data], axis=1)\n",
    "\n",
    "# drop unnecessary fullText column since we have it vectorized now\n",
    "data_unchanged = data.copy()\n",
    "data = data.drop(columns=[\"fullText\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9104217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 928, Val size: 199, Test size: 199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split it into train, val and test sets\n",
    "X, y = data[data.columns[:-1]], data[\"bps_change\"]\n",
    "\n",
    "# temporal split - no shuffling. 70-15-15 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9a6ee744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.43261037848957296, Train MSE: 888.5217877001062\n",
      "Validation R2: -2.5342832819737646, Validation MSE: 2193.427823880905\n"
     ]
    }
   ],
   "source": [
    "# Lets finally fit a linear regression model for a baseline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get metrics\n",
    "    R2_train = model.score(X_train, y_train)\n",
    "    MSE_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "\n",
    "    R2_val = model.score(X_val, y_val)\n",
    "    MSE_val = mean_squared_error(y_val, model.predict(X_val))\n",
    "\n",
    "    print(f\"Train R2: {R2_train}, Train MSE: {MSE_train}\")\n",
    "    print(f\"Validation R2: {R2_val}, Validation MSE: {MSE_val}\")\n",
    "\n",
    "\n",
    "fit_model(LinearRegression(), X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "407c0dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ridge Regression ===\n",
      "Train R2: 0.2914190912933722, Train MSE: 1109.6247655326713\n",
      "Validation R2: -0.561743056987362, Validation MSE: 969.2405508128509\n",
      "=== Lasso Regression ===\n",
      "Train R2: 0.012799328450232683, Train MSE: 1545.9382270143005\n",
      "Validation R2: -0.10192047599349885, Validation MSE: 683.8679412246878\n"
     ]
    }
   ],
   "source": [
    "# Fit regularized linear models - Ridge and Lasso\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# default hyperparameters used\n",
    "print(\"=== Ridge Regression ===\")\n",
    "fit_model(Ridge(alpha=1.0), X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"=== Lasso Regression ===\")\n",
    "fit_model(Lasso(alpha=1.0), X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fdd9f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Regressor ===\n",
      "Train R2: 0.8550634929052283, Train MSE: 226.96792381227763\n",
      "Validation R2: -0.395927940096767, Validation MSE: 866.3332674994483\n",
      "=== Gradient Boosting Regressor ===\n",
      "Train R2: 0.7503057027929305, Train MSE: 391.0167104261527\n",
      "Validation R2: -0.2978706397897539, Validation MSE: 805.4774747776116\n"
     ]
    }
   ],
   "source": [
    "# lets try some tree based algorithms - bagged and boosted approach\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "print(\"=== Random Forest Regressor ===\")\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"=== Gradient Boosting Regressor ===\")\n",
    "model = GradientBoostingRegressor(n_estimators=100)\n",
    "fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# massively overfits, but gets good training performance, so the model seems expressive\n",
    "# enough for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "936bf420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Regressor ===\n",
      "Train R2: 0.6464045595835951, Train MSE: 553.7240036309278\n",
      "Validation R2: -0.6145469039012095, Validation MSE: 1002.011389420926\n"
     ]
    }
   ],
   "source": [
    "# lets try a few regularized models by reducing max depth\n",
    "\n",
    "print(\"=== Random Forest Regressor ===\")\n",
    "model = RandomForestRegressor(n_estimators=10, max_depth=15)\n",
    "fit_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2dbd342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gradient Boosting Regressor ===\n",
      "Train R2: 0.4729049964686304, Train MSE: 825.4211516572119\n",
      "Validation R2: -0.12408173755491458, Validation MSE: 697.6215438204496\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Gradient Boosting Regressor ===\")\n",
    "model = GradientBoostingRegressor(n_estimators=10, max_depth=15, min_samples_leaf=10)\n",
    "fit_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "af43f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               fullText  bps_change\n",
      "1316                            https://t.co/VnLcmywp6g -234.034833\n",
      "1268      This is so crazy!\\n\\n https://t.co/p8B3dXrkAW -179.485418\n",
      "1269  Increasingly significant consideration https:/... -145.140419\n",
      "1266  European justice is broken \\n https://t.co/Pcf... -166.553596\n",
      "1267      Thunder Dome vibes \\n https://t.co/gYVNl1ufPS -136.544531\n",
      "1302                            https://t.co/szHlftGMee -113.597394\n",
      "1169  Unless @DOGE ends the careers of deceitful, po... -128.738769\n",
      "1170  Your elected representatives have heard you an... -124.640894\n",
      "1265  It might make sense to increase compensation f...   89.331554\n",
      "1327                            https://t.co/8thYN2waCI   98.937497\n",
      "1328                          üòâ https://t.co/jR6ycgAYjc  -93.183120\n",
      "1175                                  Sounds reasonable  -87.433179\n",
      "1155  Mind-blowing @USGAO report: https://t.co/YaA3o...  -80.030200\n",
      "1171  You are the media now\\n\\n https://t.co/kuQRkxakxh  -75.113122\n",
      "1322                            https://t.co/rQLo9ALEAL   73.495984\n",
      "1249  The REAL reason so many Democrats are upset ab...   60.845070\n",
      "1210  Me: How many radical leftists does it take to ...   68.645461\n",
      "1275  A senior exec at Tesla sent me some funny pics...  -73.267598\n",
      "1280  If daylight savings time change is canceled, d...  -78.812654\n",
      "1248                                What happened here?   60.845070\n",
      "\n",
      "--- Final Analysis of Top 20 High-Error Samples ---\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "squared_errors = (y_test - y_pred) ** 2\n",
    "overall_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "error_df = X_test.copy()\n",
    "error_df[\"True_Value\"] = y_test.values\n",
    "error_df[\"Predicted_Value\"] = y_pred\n",
    "error_df[\"Squared_Error\"] = squared_errors.values\n",
    "\n",
    "# 5. Sort the DataFrame by the Squared_Error column (descending)\n",
    "sorted_error_df = error_df.sort_values(by=\"Squared_Error\", ascending=False)\n",
    "\n",
    "# 6. Extract the top 20 rows\n",
    "top_20_errors = sorted_error_df.head(20)\n",
    "high_error_indices = top_20_errors.index.tolist()\n",
    "\n",
    "original_high_error_text = data_unchanged.loc[\n",
    "    high_error_indices, [\"fullText\", \"bps_change\"]\n",
    "]\n",
    "print(original_high_error_text)\n",
    "\n",
    "\n",
    "original_text_df = original_high_error_text\n",
    "original_text_df.columns = [\"Original_Text\", \"bps_change\"]\n",
    "\n",
    "# Merge the original text with the top_20_errors DataFrame\n",
    "# We only need the index, True_Value, Predicted_Value, and Squared_Error columns\n",
    "analysis_df = top_20_errors[[\"True_Value\", \"Predicted_Value\", \"Squared_Error\"]].copy()\n",
    "\n",
    "# Join the original text (which shares the same index)\n",
    "final_analysis_df = analysis_df.join(original_text_df)\n",
    "\n",
    "print(\"\\n--- Final Analysis of Top 20 High-Error Samples ---\")\n",
    "# Display the DataFrame, sorting by error descending again for clarity\n",
    "# print(final_analysis_df.sort_values(by=\"Squared_Error\", ascending=False))\n",
    "final_analysis_df.to_csv(\"high_error.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ddb68b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               fullText  bps_change\n",
      "1132  A fully reusable rocket with orbital refilling...  -10.157137\n",
      "1309  Extremely important difference https://t.co/yu...   -0.718313\n",
      "1165  Testing streaming on X via Starlink https://t....   -8.155583\n",
      "1308                      Yup\\n https://t.co/B8m7hTmND7   -7.145664\n",
      "1180  Naturally, I would NEVER be an attention-seeki...    8.963904\n",
      "1228   Greatest President ever! https://t.co/U8BuPVwSwd    8.590143\n",
      "1323                          üòÇ https://t.co/gBcA9SLl1a    1.838506\n",
      "1150  It was a coordinated sychological operation \\n...    4.166419\n",
      "1234           Check out my bio https://t.co/vAUdrHVDW5    1.504031\n",
      "1152                                    Defund the ACLU   -6.210479\n",
      "1273  President @realDonaldTrump is the Commander-in...  -10.187592\n",
      "1288        So much fake gnus\\n https://t.co/BGZQiMhhX0    5.211674\n",
      "1136  So many life lessons to be learned from speedr...   -2.055076\n",
      "1189        Testing X Streaming https://t.co/f8LmyeSUHR   -3.011443\n",
      "1208  We should expand consciousness to the stars, s...   -6.693920\n",
      "1140               Falcon Heavy https://t.co/ypeJNo6KP3   -0.291622\n",
      "1242  Federal judges who repeatedly abuse their auth...  -13.642565\n",
      "1235             Now 69% off!\\n https://t.co/PkZStVGo9K   13.805522\n",
      "1270  There is a shortage of top notch air traffic c...  -28.043776\n",
      "1300              Good question https://t.co/VXfXS3sdoC   -0.212202\n",
      "1329                        üòÇ\\n https://t.co/pQCzmRMrah   -0.402156\n",
      "1303                            https://t.co/6clis9g02E    3.807429\n",
      "1141       They want our memes! https://t.co/jx42ecApTs   -2.623677\n",
      "1272  Watch this carefully. Very important. \\n https...    0.351865\n",
      "1299                            https://t.co/QHSUaAQmB7   -2.123593\n",
      "1181  Out of curiosity, I glanced through the legacy...  -12.103664\n",
      "1204  Actual footage of @ElonMusk &amp; @JeffBezos a...   -9.937948\n",
      "1314                            https://t.co/gqVRSfMxp4    1.085776\n",
      "1237    ‚ÄúJudicial dicktatorship is wrong!‚Äù ‚Äì Harry B≈çlz   12.589928\n",
      "1313                            https://t.co/27B9U2S0Mq   -6.158528\n",
      "1142  The legacy media forgot that honesty really is...   -3.427005\n",
      "1282    Very productive meeting https://t.co/RrMYb3CGh5    0.380575\n",
      "1298                            https://t.co/xKM3yZ0iGk    2.540866\n",
      "1260  Great President @realDonaldTrump \\n https://t....    1.182872\n",
      "1238  They imported voters to stack the election\\n h...  -11.203634\n",
      "1319                  Amazing ü§£ https://t.co/y6a4EE8mVt    4.574391\n",
      "1225  Will test Starlink streaming again late tonigh...   -5.534764\n",
      "1315                            https://t.co/lzxsf8vyd5    6.218678\n",
      "1131  The objectives for Starship Flight 6 are:\\n\\n1...    6.253722\n",
      "1205  Starship Flight 7 launches in 2 hours\\n https:...   -4.695740\n",
      "1317  On Sunday night, I will give a talk in Wiscons...    1.900563\n",
      "1198  If you‚Äôre a hardcore software engineer and wan...   -3.657020\n",
      "1287  Please post feedback for improving the ùïè algor...   11.483009\n",
      "1138                                           gm frens    4.665675\n",
      "1144  Making life multiplanetary for the first time ...   -4.157510\n",
      "1262  Play to win decisively \\n https://t.co/BViMbfRglZ   -9.794610\n",
      "1212  Live broadcast from The @WhiteHouse. \\n\\nYou c...  -14.469943\n",
      "1220             Would you like @DOGE to audit the IRS?  -14.747355\n",
      "1135  Milton Friedman was the best \\n https://t.co/C...    0.000000\n",
      "1162            Haha amazing üî•üî• https://t.co/Ck8wFQV5KU   -2.771363\n",
      "\n",
      "--- Final Analysis of Bottom 50 Low-Error Samples ---\n"
     ]
    }
   ],
   "source": [
    "sorted_error_df = error_df.sort_values(by=\"Squared_Error\", ascending=True)\n",
    "bottom_50_errors = sorted_error_df.head(50)\n",
    "low_error_indices = bottom_50_errors.index.tolist()\n",
    "\n",
    "original_low_error_text = data_unchanged.loc[\n",
    "    low_error_indices, [\"fullText\", \"bps_change\"]\n",
    "]\n",
    "print(original_low_error_text)\n",
    "\n",
    "original_text_df = original_low_error_text\n",
    "original_text_df.columns = [\"Original_Text\", \"bps_change\"]\n",
    "\n",
    "# Merge the original text with the top_20_errors DataFrame\n",
    "# We only need the index, True_Value, Predicted_Value, and Squared_Error columns\n",
    "analysis_df = bottom_50_errors[\n",
    "    [\"True_Value\", \"Predicted_Value\", \"Squared_Error\"]\n",
    "].copy()\n",
    "\n",
    "# Join the original text (which shares the same index)\n",
    "final_analysis_df = analysis_df.join(original_text_df)\n",
    "\n",
    "print(\"\\n--- Final Analysis of Bottom 50 Low-Error Samples ---\")\n",
    "# Display the DataFrame, sorting by error descending again for clarity\n",
    "# print(final_analysis_df.sort_values(by=\"Squared_Error\", ascending=False))\n",
    "final_analysis_df.to_csv(\"low_error.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
